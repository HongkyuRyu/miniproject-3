{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-22T06:49:31.817967Z","iopub.execute_input":"2023-03-22T06:49:31.818354Z","iopub.status.idle":"2023-03-22T06:49:31.834236Z","shell.execute_reply.started":"2023-03-22T06:49:31.818315Z","shell.execute_reply":"2023-03-22T06:49:31.833272Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\ndef make_dataframe():\n\n    # 절대경로\n    path = []\n    # train, val, test data\n    dataset_gubuns = []\n    # abnormal or normal 구분\n    labels_gubuns = []\n\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            if '.png' in filename:\n                file_path = dirname + '/' + filename\n                path.append(file_path)\n            if '/train/' in file_path:\n                dataset_gubuns.append('train')\n            elif '/val/' in file_path:\n                dataset_gubuns.append('val')\n            else:\n                dataset_gubuns.append('test')\n\n            if '/abnormal/' in file_path:\n                labels_gubuns.append('abnormal')\n            elif '/normal/' in file_path:\n                labels_gubuns.append('normal')\n    data_df = pd.DataFrame({'path': path, 'dataset': dataset_gubuns,'label': labels_gubuns})\n    return data_df","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:49:35.097221Z","iopub.execute_input":"2023-03-22T06:49:35.097923Z","iopub.status.idle":"2023-03-22T06:49:35.106013Z","shell.execute_reply.started":"2023-03-22T06:49:35.097885Z","shell.execute_reply":"2023-03-22T06:49:35.104747Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 200)\ndata_df = make_dataframe()\nprint('data_df shape :' ,data_df.shape)\ndata_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:49:38.724782Z","iopub.execute_input":"2023-03-22T06:49:38.725799Z","iopub.status.idle":"2023-03-22T06:49:38.749405Z","shell.execute_reply.started":"2023-03-22T06:49:38.725743Z","shell.execute_reply":"2023-03-22T06:49:38.748090Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"data_df shape : (605, 3)\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                                                                                       path  \\\n0  /kaggle/input/output/val/normal/DALLíñE 2023-03-10 23.42.04 - photo of a part of car without blemish.png   \n1  /kaggle/input/output/val/normal/DALLíñE 2023-03-10 23.34.47 - photo of a part of car without blemish.png   \n2                  /kaggle/input/output/val/normal/DALLíñE 2023-03-11 00.57.59 - photo of a part of car.png   \n3                  /kaggle/input/output/val/normal/DALLíñE 2023-03-11 14.41.37 - photo of part of a car.png   \n4                         /kaggle/input/output/val/normal/DALLíñE 2023-03-11 01.16.06 - a part of a car.png   \n\n  dataset   label  \n0     val  normal  \n1     val  normal  \n2     val  normal  \n3     val  normal  \n4     val  normal  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>dataset</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/output/val/normal/DALLíñE 2023-03-10 23.42.04 - photo of a part of car without blemish.png</td>\n      <td>val</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/output/val/normal/DALLíñE 2023-03-10 23.34.47 - photo of a part of car without blemish.png</td>\n      <td>val</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/output/val/normal/DALLíñE 2023-03-11 00.57.59 - photo of a part of car.png</td>\n      <td>val</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/output/val/normal/DALLíñE 2023-03-11 14.41.37 - photo of part of a car.png</td>\n      <td>val</td>\n      <td>normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/output/val/normal/DALLíñE 2023-03-11 01.16.06 - a part of a car.png</td>\n      <td>val</td>\n      <td>normal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Sequence class로 작성","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nimport sklearn \nimport cv2\n\n\nBATCH_SIZE = 32\nIMAGE_SIZE = 512\n\n \nclass CDSequence(Sequence):\n    def __init__(self, filenames, labels, batch_size=32, aug=None, shuffle=False):\n    # image의 절대경로들\n        self.filenames = filenames\n        self.labels = labels\n        self.batch_size = batch_size\n        # albumentation 객체\n        self.aug = aug\n        self.shuffle = shuffle\n\n        # 훈련 데이터의 경우\n        if self.shuffle:\n            self.on_epoch_end()\n\n    def __len__(self):\n    # 총 step의 갯수\n        return len(self.labels) // self.batch_size\n\n    def __getitem__(self, index):\n    # 현재 인덱스를 기준으로 batch_size만큼 데이터를 가져옴\n        meta_data = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n        # 훈련, 검증 데이터세트인 경우\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n\n        # 불러온 meta_data를 np.array로 저장할 빈 공간을 생성\n        image_batch = np.zeros((meta_data.shape[0], 512, 512, 3))\n        for i in range(meta_data.shape[0]):\n            # cv2는 이미지를 BGR로 불러오기 때문이 RGB로 바꾸어줌\n            image = cv2.cvtColor(cv2.imread(meta_data[i]), cv2.COLOR_BGR2RGB)\n            # 이미지의 크기가 전부 다르기 때문에 통일 시켜 주어야함\n            image = cv2.resize(image, (512, 512))\n            # augmentation이 있으면 적용\n            if self.aug is not None:\n                image = self.aug(image=image)['image']\n            \n            # 빈 이미지 배치에 최종 이미지를 등록\n            image_batch[i] = image\n        \n        return image_batch, label_batch\n\n    def on_epoch_end(self):\n        # 파일과 라벨을 같이 섞어 주어야한다.\n        if self.shuffle:\n            self.image_filenames, self.labels = sklearn.utils.shuffle(self.filenames, self.labels)\n        else:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:49:43.508670Z","iopub.execute_input":"2023-03-22T06:49:43.509619Z","iopub.status.idle":"2023-03-22T06:49:43.521491Z","shell.execute_reply.started":"2023-03-22T06:49:43.509559Z","shell.execute_reply":"2023-03-22T06:49:43.520379Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import albumentations as A\n\ntrain_df = data_df[data_df['dataset'] == 'train']\nval_df = data_df[data_df['dataset'] == 'val']\ntest_df = data_df[data_df['dataset'] == 'test']\n\ntr_path = train_df['path'].values\ntr_label = pd.factorize(train_df['label'])[0]\n\nval_path = val_df['path'].values\nval_label = pd.factorize(val_df['label'])[0]\n\ntest_path = test_df['path'].values\ntest_label = pd.factorize(test_df['label'])[0]\n\n\naugmentor_01 = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(scale_limit=(0.5, 0.9), p=0.5, rotate_limit=30),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n    A.Blur(p=0.2),\n    \n    \n])\n\n# CD_dataset = CDSequence(tr_path, tr_labels, batch_size=32, aug=augmentor_01, shuffle=False)\n# show_first_data(CD_dataset, image_verbose=True)\n\nprint(tr_path[:5], tr_label[:5])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:00.391253Z","iopub.execute_input":"2023-03-22T06:50:00.391631Z","iopub.status.idle":"2023-03-22T06:50:00.407885Z","shell.execute_reply.started":"2023-03-22T06:50:00.391599Z","shell.execute_reply":"2023-03-22T06:50:00.406447Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"['/kaggle/input/output/train/normal/DALLíñE 2023-03-11 00.55.42 - photo of a part of car.png'\n '/kaggle/input/output/train/normal/DALLíñE 2023-03-11 01.04.49 - photo of a part of car.png'\n '/kaggle/input/output/train/normal/DALLíñE 2023-03-11 14.25.21 - part of a car.png'\n '/kaggle/input/output/train/normal/DALLíñE 2023-03-11 14.32.34 - part of a car.png'\n '/kaggle/input/output/train/normal/DALLíñE 2023-03-11 14.22.54 - part of a car.png'] [0 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.applications.xception import preprocess_input\nimport sklearn\nimport cv2\n\nclass CDSequence(Sequence):\n    def __init__(self, filenames, labels, batch_size=32, aug=None, shuffle=False, pre_func=None):\n        self.filenames = filenames\n        self.labels = labels\n        self.batch_size = batch_size\n        self.aug = aug\n        self.shuffle = shuffle\n        self.pre_func = pre_func\n\n        if self.shuffle:\n            self.on_epoch_end()\n\n    def __len__(self):\n        return len(self.labels) // self.batch_size\n\n    def __getitem__(self, index):\n        meta_data = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n        # 훈련, 검증 데이터세트인 경우\n        if self.labels is not None:\n            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n\n        # 불러온 meta_data를 np.array로 저장할 빈 공간을 생성\n        image_batch = np.zeros((meta_data.shape[0], 512, 512, 3), dtype='float32')\n        for i in range(meta_data.shape[0]):\n            # cv2는 이미지를 BGR로 불러오기 때문이 RGB로 바꾸어줌\n            image = cv2.cvtColor(cv2.imread(meta_data[i]), cv2.COLOR_BGR2RGB)\n            # 이미지의 크기가 전부 다르기 때문에 통일 시켜 주어야함\n            image = cv2.resize(image, (512, 512))\n            # augmentation이 있으면 적용\n            if self.aug is not None:\n                image = self.aug(image=image)['image']\n            \n            # 이미지 값을 self.pre_func 함수로 스케일링\n            if self.pre_func is not None:\n                image = self.pre_func(image)\n\n            # 빈 이미지 배치에 최종 이미지를 등록\n            image_batch[i] = image\n        \n        return image_batch, label_batch\n\n    def on_epoch_end(self):\n        # 파일과 라벨을 같이 섞어 주어야한다.\n        if self.shuffle:\n            self.filenames, self.labels = sklearn.utils.shuffle(self.filenames, self.labels)\n        else:\n            pass","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:12.358451Z","iopub.execute_input":"2023-03-22T06:50:12.358802Z","iopub.status.idle":"2023-03-22T06:50:12.372161Z","shell.execute_reply.started":"2023-03-22T06:50:12.358772Z","shell.execute_reply":"2023-03-22T06:50:12.371037Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess_input\n\ntr_dataset = CDSequence(tr_path, tr_label, batch_size=32, aug=augmentor_01, shuffle=True, pre_func=eff_preprocess_input)\nval_dataset = CDSequence(val_path, val_label, batch_size=32, aug=None, shuffle=False, pre_func=eff_preprocess_input)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:18.334966Z","iopub.execute_input":"2023-03-22T06:50:18.335641Z","iopub.status.idle":"2023-03-22T06:50:18.342205Z","shell.execute_reply.started":"2023-03-22T06:50:18.335601Z","shell.execute_reply":"2023-03-22T06:50:18.341017Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.applications import Xception, ResNet50V2, EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3\nfrom tensorflow.keras.applications import MobileNet, MobileNetV2\nfrom tensorflow.keras.applications import EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\nimport tensorflow as tf\n\ndef create_model(model_type='xception', verbose=False):\n    # EfficientNet으로 한번 시도해보자. 안해본 모델도 모두 추가.\n    \n    input_tensor = Input(shape=(512, 512, 3))\n    if model_type == 'resnet50v2':\n        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'xception':\n        base_model = tf.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb1':\n        base_model = tf.keras.applications.EfficientNetB1(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb2':\n        base_model = tf.keras.applications.EfficientNetB2(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb3':\n        base_model = tf.keras.applications.EfficientNetB3(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb4':\n        base_model = tf.keras.applications.EfficientNetB4(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb5':\n        base_model = tf.keras.applications.EfficientNetB5(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb6':\n        base_model = tf.keras.applications.EfficientNetB6(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    elif model_type == 'efficientnetb7':\n        base_model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet', input_tensor=input_tensor)\n    \n    bm_output = base_model.output\n    x = GlobalAveragePooling2D()(bm_output)\n    x = Dense(64, activation='relu', name='fc1')(x)\n    x = Dropout(rate=0.5)(x)\n    output = Dense(1, activation='sigmoid', name='output')(x)\n    \n    model = Model(input_tensor, output)\n    \n    if verbose:\n        model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:23.233239Z","iopub.execute_input":"2023-03-22T06:50:23.233860Z","iopub.status.idle":"2023-03-22T06:50:23.249415Z","shell.execute_reply.started":"2023-03-22T06:50:23.233822Z","shell.execute_reply":"2023-03-22T06:50:23.248461Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model = create_model(model_type='efficientnetb3', verbose=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:27.371529Z","iopub.execute_input":"2023-03-22T06:50:27.371916Z","iopub.status.idle":"2023-03-22T06:50:31.071751Z","shell.execute_reply.started":"2023-03-22T06:50:27.371883Z","shell.execute_reply":"2023-03-22T06:50:31.070656Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"print(f'모델의 레이어수: {len(model.layers)}')","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:34.789377Z","iopub.execute_input":"2023-03-22T06:50:34.790620Z","iopub.status.idle":"2023-03-22T06:50:34.797927Z","shell.execute_reply.started":"2023-03-22T06:50:34.790554Z","shell.execute_reply":"2023-03-22T06:50:34.796466Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"모델의 레이어수: 389\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 1. Effecient Net B0 model fine tuning","metadata":{}},{"cell_type":"code","source":"for layer in model.layers[:-4]:\n    layer.trainable = False\nfor layer in model.layers[-4:]:\n    layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:50:57.824184Z","iopub.execute_input":"2023-03-22T06:50:57.825190Z","iopub.status.idle":"2023-03-22T06:50:57.844199Z","shell.execute_reply.started":"2023-03-22T06:50:57.825133Z","shell.execute_reply":"2023-03-22T06:50:57.843038Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"for idx, layer in enumerate(model.layers):\n    if idx < 100:\n        layer.trainable = False\n    else:\n        layer.trainable = True","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:42:30.431286Z","iopub.execute_input":"2023-03-22T06:42:30.432513Z","iopub.status.idle":"2023-03-22T06:42:30.450281Z","shell.execute_reply.started":"2023-03-22T06:42:30.432475Z","shell.execute_reply":"2023-03-22T06:42:30.449205Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\nlr_reduction = ReduceLROnPlateau(monitor='val_loss',\n                                patience=4,\n                                verbose=1,\n                                factor=0.2,\n                                min_lr = 0.000001)\nes = EarlyStopping(monitor='val_loss',\n                  min_delta = 0,\n                  patience=5,\n                  verbose=1,\n                  restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:51:03.538704Z","iopub.execute_input":"2023-03-22T06:51:03.539883Z","iopub.status.idle":"2023-03-22T06:51:03.561318Z","shell.execute_reply.started":"2023-03-22T06:51:03.539834Z","shell.execute_reply":"2023-03-22T06:51:03.560395Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"tr_dataset","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:51:06.634734Z","iopub.execute_input":"2023-03-22T06:51:06.635126Z","iopub.status.idle":"2023-03-22T06:51:06.643468Z","shell.execute_reply.started":"2023-03-22T06:51:06.635081Z","shell.execute_reply":"2023-03-22T06:51:06.642126Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<__main__.CDSequence at 0x7f1464104b50>"},"metadata":{}}]},{"cell_type":"code","source":"First_epochs = 100\nsecond_epochs = 100\n\nhist = model.fit(tr_dataset, epochs=First_epochs, validation_data=val_dataset,\n                verbose=1, callbacks=[es, lr_reduction])\n\n# for layer in model.layers:\n#     if not isinstance(layer):\n#         layer.trainable = True\n# model.compile(optimizer=Adam(0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n# hist = model.fit(tr_dataset, epochs=second_epochs, validation_data=val_dataset,\n#                 verbose=1, callbacks=[es, lr_reduction])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:51:11.355661Z","iopub.execute_input":"2023-03-22T06:51:11.356026Z","iopub.status.idle":"2023-03-22T06:55:56.048366Z","shell.execute_reply.started":"2023-03-22T06:51:11.355994Z","shell.execute_reply":"2023-03-22T06:55:56.047372Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"2023-03-22 06:51:22.182547: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"11/11 [==============================] - 36s 2s/step - loss: 0.3863 - accuracy: 0.8409 - val_loss: 0.2358 - val_accuracy: 0.9375 - lr: 0.0010\nEpoch 2/100\n11/11 [==============================] - 20s 2s/step - loss: 0.1646 - accuracy: 0.9489 - val_loss: 0.1564 - val_accuracy: 0.9479 - lr: 0.0010\nEpoch 3/100\n11/11 [==============================] - 20s 2s/step - loss: 0.1304 - accuracy: 0.9375 - val_loss: 0.1847 - val_accuracy: 0.9375 - lr: 0.0010\nEpoch 4/100\n11/11 [==============================] - 21s 2s/step - loss: 0.0745 - accuracy: 0.9744 - val_loss: 0.1214 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 5/100\n11/11 [==============================] - 21s 2s/step - loss: 0.0834 - accuracy: 0.9744 - val_loss: 0.1176 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 6/100\n11/11 [==============================] - 20s 2s/step - loss: 0.0771 - accuracy: 0.9773 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 7/100\n11/11 [==============================] - 21s 2s/step - loss: 0.0529 - accuracy: 0.9830 - val_loss: 0.1144 - val_accuracy: 0.9688 - lr: 0.0010\nEpoch 8/100\n11/11 [==============================] - 21s 2s/step - loss: 0.0444 - accuracy: 0.9858 - val_loss: 0.1016 - val_accuracy: 0.9792 - lr: 0.0010\nEpoch 9/100\n11/11 [==============================] - 20s 2s/step - loss: 0.0402 - accuracy: 0.9915 - val_loss: 0.1074 - val_accuracy: 0.9792 - lr: 0.0010\nEpoch 10/100\n11/11 [==============================] - 21s 2s/step - loss: 0.0337 - accuracy: 0.9886 - val_loss: 0.1129 - val_accuracy: 0.9688 - lr: 0.0010\nEpoch 11/100\n11/11 [==============================] - 20s 2s/step - loss: 0.0505 - accuracy: 0.9830 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 12/100\n11/11 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9659\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n11/11 [==============================] - 20s 2s/step - loss: 0.0671 - accuracy: 0.9659 - val_loss: 0.1195 - val_accuracy: 0.9688 - lr: 0.0010\nEpoch 13/100\n11/11 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9830Restoring model weights from the end of the best epoch: 8.\n11/11 [==============================] - 21s 2s/step - loss: 0.0415 - accuracy: 0.9830 - val_loss: 0.1187 - val_accuracy: 0.9688 - lr: 2.0000e-04\nEpoch 13: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"layers","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:30:51.910258Z","iopub.execute_input":"2023-03-22T06:30:51.911155Z","iopub.status.idle":"2023-03-22T06:30:51.938185Z","shell.execute_reply.started":"2023-03-22T06:30:51.911105Z","shell.execute_reply":"2023-03-22T06:30:51.936901Z"},"trusted":true},"execution_count":81,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/366190394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"],"ename":"NameError","evalue":"name 'layers' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# hist = model.fit(tr_dataset, epochs=1000, validation_data=val_dataset,\n#                 verbose=1, callbacks=[es, lr_reduction])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:36:18.650127Z","iopub.execute_input":"2023-03-22T05:36:18.651302Z","iopub.status.idle":"2023-03-22T05:41:53.741004Z","shell.execute_reply.started":"2023-03-22T05:36:18.651248Z","shell.execute_reply":"2023-03-22T05:41:53.740000Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n","output_type":"stream"},{"name":"stderr","text":"2023-03-22 05:36:31.622751: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"11/11 [==============================] - 48s 2s/step - loss: 0.3867 - accuracy: 0.8409 - val_loss: 0.2713 - val_accuracy: 0.9375 - lr: 0.0010\nEpoch 2/1000\n11/11 [==============================] - 21s 2s/step - loss: 0.1806 - accuracy: 0.9460 - val_loss: 0.2532 - val_accuracy: 0.9688 - lr: 0.0010\nEpoch 3/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.1064 - accuracy: 0.9716 - val_loss: 0.8223 - val_accuracy: 0.9271 - lr: 0.0010\nEpoch 4/1000\n11/11 [==============================] - 21s 2s/step - loss: 0.0871 - accuracy: 0.9716 - val_loss: 0.4350 - val_accuracy: 0.9479 - lr: 0.0010\nEpoch 5/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.1105 - accuracy: 0.9659 - val_loss: 0.1682 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 6/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.0286 - accuracy: 0.9858 - val_loss: 0.1656 - val_accuracy: 0.9479 - lr: 0.0010\nEpoch 7/1000\n11/11 [==============================] - 21s 2s/step - loss: 0.0406 - accuracy: 0.9801 - val_loss: 0.4328 - val_accuracy: 0.9479 - lr: 0.0010\nEpoch 8/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.0252 - accuracy: 0.9886 - val_loss: 1.5954 - val_accuracy: 0.8958 - lr: 0.0010\nEpoch 9/1000\n11/11 [==============================] - 21s 2s/step - loss: 0.0927 - accuracy: 0.9631 - val_loss: 0.7157 - val_accuracy: 0.9271 - lr: 0.0010\nEpoch 10/1000\n11/11 [==============================] - 21s 2s/step - loss: 0.1049 - accuracy: 0.9631 - val_loss: 0.0909 - val_accuracy: 0.9792 - lr: 0.0010\nEpoch 11/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.0575 - accuracy: 0.9886 - val_loss: 0.1398 - val_accuracy: 0.9688 - lr: 0.0010\nEpoch 12/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.0384 - accuracy: 0.9915 - val_loss: 0.2103 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 13/1000\n11/11 [==============================] - 20s 2s/step - loss: 0.0561 - accuracy: 0.9830 - val_loss: 0.1124 - val_accuracy: 0.9583 - lr: 0.0010\nEpoch 14/1000\n11/11 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9858\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n11/11 [==============================] - 20s 2s/step - loss: 0.0552 - accuracy: 0.9858 - val_loss: 0.6903 - val_accuracy: 0.8750 - lr: 0.0010\nEpoch 15/1000\n11/11 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9830Restoring model weights from the end of the best epoch: 10.\n11/11 [==============================] - 21s 2s/step - loss: 0.0714 - accuracy: 0.9830 - val_loss: 0.3830 - val_accuracy: 0.9062 - lr: 2.0000e-04\nEpoch 15: early stopping\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### efficient B0","metadata":{}},{"cell_type":"code","source":"test_dataset = CDSequence(test_path, test_label, batch_size=32, aug=None, shuffle=False, pre_func=eff_preprocess_input)\nres = model.evaluate(test_dataset)\nprint('loss: ', res[0], 'accuracy: ', res[1])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T05:45:21.102252Z","iopub.execute_input":"2023-03-22T05:45:21.102852Z","iopub.status.idle":"2023-03-22T05:45:28.073938Z","shell.execute_reply.started":"2023-03-22T05:45:21.102815Z","shell.execute_reply":"2023-03-22T05:45:28.072636Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 5s 2s/step - loss: 0.0868 - accuracy: 0.9896\nloss:  0.08679777383804321 accuracy:  0.9895833134651184\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = CDSequence(test_path, test_label, batch_size=32, aug=None, shuffle=False, pre_func=eff_preprocess_input)\nres = model.evaluate(test_dataset)\nprint('loss: ', res[0], 'accuracy: ', res[1])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:34:46.478103Z","iopub.execute_input":"2023-03-22T06:34:46.478545Z","iopub.status.idle":"2023-03-22T06:34:53.163945Z","shell.execute_reply.started":"2023-03-22T06:34:46.478509Z","shell.execute_reply":"2023-03-22T06:34:53.162751Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 4s 1s/step - loss: 0.0379 - accuracy: 0.9792\nloss:  0.03790709376335144 accuracy:  0.9791666865348816\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = CDSequence(test_path, test_label, batch_size=32, aug=None, shuffle=False, pre_func=eff_preprocess_input)\nres = model.evaluate(test_dataset)\nprint('loss: ', res[0], 'accuracy: ', res[1])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:49:02.544257Z","iopub.execute_input":"2023-03-22T06:49:02.545483Z","iopub.status.idle":"2023-03-22T06:49:09.363819Z","shell.execute_reply.started":"2023-03-22T06:49:02.545440Z","shell.execute_reply":"2023-03-22T06:49:09.362549Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 5s 2s/step - loss: 0.2230 - accuracy: 0.9583\nloss:  0.22295747697353363 accuracy:  0.9583333134651184\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### EfficientNetB3\n-   A.HorizontalFlip(p=0.5),\n-   A.VerticalFlip(p=0.5),\n-   A.ShiftScaleRotate(scale_limit=(0.5, 0.9), p=0.5, rotate_limit=30),\n-   A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n-   A.Blur(p=0.2),","metadata":{}},{"cell_type":"code","source":"test_dataset = CDSequence(test_path, test_label, batch_size=32, aug=None, shuffle=False, pre_func=eff_preprocess_input)\nres = model.evaluate(test_dataset)\nprint('loss: ', res[0], 'accuracy: ', res[1])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T06:56:13.019604Z","iopub.execute_input":"2023-03-22T06:56:13.020204Z","iopub.status.idle":"2023-03-22T06:56:19.449940Z","shell.execute_reply.started":"2023-03-22T06:56:13.020167Z","shell.execute_reply":"2023-03-22T06:56:19.448949Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 5s 1s/step - loss: 0.0377 - accuracy: 0.9896\nloss:  0.03771309554576874 accuracy:  0.9895833134651184\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}